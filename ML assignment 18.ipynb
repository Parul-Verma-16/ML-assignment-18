{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d822ae",
   "metadata": {},
   "source": [
    "## 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393e3e7",
   "metadata": {},
   "source": [
    "**Supervised Learning:**\n",
    "Supervised learning is a type of machine learning where the algorithm learns from labeled training data, which means the input data is accompanied by corresponding output labels. The goal of supervised learning is to learn a mapping between input features and the correct output labels so that the model can make accurate predictions on unseen data.\n",
    "\n",
    "Examples of supervised learning tasks:\n",
    "1. **Image Classification:** Given a dataset of images with corresponding labels (e.g., cats or dogs), the algorithm learns to classify new images into one of the predefined classes.\n",
    "2. **Speech Recognition:** In speech recognition, the algorithm learns from audio recordings with transcriptions to recognize spoken words and convert them into text.\n",
    "3. **Regression Analysis:** Predicting house prices based on features like square footage, number of bedrooms, and location using historical sales data.\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "Unsupervised learning, on the other hand, is a type of machine learning where the algorithm learns from unlabeled data, meaning there are no corresponding output labels provided. The objective of unsupervised learning is to identify patterns or structures within the data without explicit guidance on the correct outputs.\n",
    "\n",
    "Examples of unsupervised learning tasks:\n",
    "1. **Clustering:** Grouping similar data points together based on their inherent patterns or similarities, such as customer segmentation for targeted marketing.\n",
    "2. **Dimensionality Reduction:** Reducing the number of input features while retaining essential information, like Principal Component Analysis (PCA).\n",
    "3. **Anomaly Detection:** Identifying rare or unusual data points that deviate significantly from the majority of the data.\n",
    "\n",
    "**Illustrative Examples:**\n",
    "1. **Supervised Learning:** Consider a spam email classifier. The algorithm is trained on a labeled dataset with features extracted from emails and corresponding labels (spam or not spam). It learns to differentiate between spam and non-spam emails based on the input features and the provided labels. Once trained, it can predict whether a new email is spam or not with high accuracy.\n",
    "\n",
    "2. **Unsupervised Learning:** In an e-commerce website, the site owner wants to segment customers into distinct groups based on their purchase behavior. They apply a clustering algorithm to analyze the purchase history of customers (unlabeled data) and identify groups of customers who exhibit similar buying patterns. The algorithm groups customers without any predefined categories, helping the site owner understand different customer segments for targeted marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39086045",
   "metadata": {},
   "source": [
    "## 2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ee135",
   "metadata": {},
   "source": [
    "Here are a few unsupervised learning applications:\n",
    "\n",
    "1. **Clustering:** Grouping similar data points together based on their similarities. This is used in customer segmentation, where customers are grouped into clusters with similar buying behavior, allowing businesses to tailor marketing strategies to each segment.\n",
    "\n",
    "2. **Anomaly Detection:** Identifying rare or unusual data points that deviate significantly from the majority of the data. This is used in fraud detection, network intrusion detection, and equipment failure prediction.\n",
    "\n",
    "3. **Dimensionality Reduction:** Reducing the number of input features while retaining essential information. Techniques like Principal Component Analysis (PCA) and t-SNE are used for visualizing high-dimensional data and compressing data for more efficient processing.\n",
    "\n",
    "4. **Recommendation Systems:** Recommending products, movies, or articles to users based on their past preferences and behavior. Unsupervised learning techniques can be used to create user-item embeddings and find similar items or users for personalized recommendations.\n",
    "\n",
    "5. **Image Compression:** Reducing the size of images without significantly losing visual information. Autoencoders can be used to compress images and later reconstruct them with minimal loss.\n",
    "\n",
    "6. **Natural Language Processing (NLP):** Topic modeling is an unsupervised learning technique used in NLP to discover topics in a collection of text documents without any predefined categories.\n",
    "\n",
    "7. **Clustering in Social Networks:** Identifying communities or groups of users with similar interests or connections in social network analysis.\n",
    "\n",
    "8. **Gene Expression Analysis:** Identifying patterns and grouping genes with similar expression profiles in genomics research.\n",
    "\n",
    "These are just a few examples, and unsupervised learning has applications in various domains where discovering patterns, similarities, or structures in data is essential for making sense of the underlying information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197114f",
   "metadata": {},
   "source": [
    "## 3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e974f",
   "metadata": {},
   "source": [
    "The three main types of clustering methods are:\n",
    "\n",
    "1. **Hierarchical Clustering:**\n",
    "   - Characteristics: Hierarchical clustering creates a tree-like structure of clusters, known as a dendrogram, by iteratively merging or splitting clusters based on their similarity or distance. The process can be agglomerative (bottom-up) or divisive (top-down).\n",
    "   - Advantages: Hierarchical clustering provides a clear visualization of the cluster hierarchy, making it easier to understand the relationships between clusters at different levels of granularity.\n",
    "   - Disadvantages: As the number of data points increases, hierarchical clustering becomes computationally expensive, and the dendrogram can become difficult to interpret for large datasets.\n",
    "\n",
    "2. **Partitioning Clustering:**\n",
    "   - Characteristics: Partitioning clustering aims to partition the data into a predefined number of clusters. Each data point belongs to one and only one cluster, and the clusters are formed by optimizing an objective function, often based on minimizing the intra-cluster distance and maximizing the inter-cluster distance.\n",
    "   - Advantages: Partitioning methods are efficient and can handle large datasets. They are straightforward to implement and can scale well.\n",
    "   - Disadvantages: Partitioning methods depend on the initial seed points, and different initializations can lead to different clustering results. They may also get stuck in local optima.\n",
    "\n",
    "3. **Density-Based Clustering:**\n",
    "   - Characteristics: Density-based clustering groups data points based on their density and connectivity. It identifies regions of high density as clusters and separates them by low-density regions. Examples include DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\n",
    "   - Advantages: Density-based clustering can handle clusters of arbitrary shapes and is robust to outliers and noise in the data. It does not require specifying the number of clusters in advance.\n",
    "   - Disadvantages: Density-based clustering can struggle with datasets of varying density or where the density difference between clusters is small.\n",
    "\n",
    "Each clustering method has its strengths and weaknesses, and the choice of which method to use depends on the nature of the data and the specific problem at hand. It's often helpful to experiment with different clustering algorithms and evaluate their results to find the most suitable approach for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e472f",
   "metadata": {},
   "source": [
    "## 4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3fc07",
   "metadata": {},
   "source": [
    "The k-means algorithm determines the consistency of clustering by iteratively assigning data points to clusters and updating the cluster centroids until convergence. The consistency of clustering is measured by the optimization objective, which is to minimize the total within-cluster sum of squared distances (also known as inertia or distortion).\n",
    "\n",
    "Here's a step-by-step explanation of how k-means achieves clustering consistency:\n",
    "\n",
    "1. **Initialization:** The algorithm starts by randomly initializing K cluster centroids, where K is the number of clusters specified by the user.\n",
    "\n",
    "2. **Assignment Step:** Each data point is assigned to the nearest cluster centroid based on the Euclidean distance (or other distance metrics) between the data point and the centroids. This step creates K clusters with data points that are closer to their respective centroids.\n",
    "\n",
    "3. **Update Step:** After the assignment step, the centroids of the K clusters are updated by calculating the mean (center) of all the data points belonging to each cluster. This moves the centroids closer to the center of their respective clusters.\n",
    "\n",
    "4. **Convergence Check:** The algorithm checks for convergence by calculating the difference between the previous centroids and the updated centroids. If the change is below a predefined threshold (a small value), the algorithm stops iterating, and the clustering is considered consistent.\n",
    "\n",
    "5. **Repeat Assignment and Update:** If the convergence criterion is not met, the algorithm repeats the assignment step and the update step until convergence is achieved.\n",
    "\n",
    "The consistency of clustering is evaluated based on the optimization objective, which is the minimized within-cluster sum of squared distances. A lower inertia value indicates better clustering consistency, where data points within the same cluster are closer to their centroid, and data points in different clusters are further apart.\n",
    "\n",
    "It's important to note that the k-means algorithm is sensitive to the initial placement of centroids. To overcome this, k-means is often run multiple times with different random initializations, and the result with the lowest inertia value is selected as the final clustering solution. Additionally, the number of clusters (K) needs to be chosen carefully, as an inappropriate choice of K can lead to suboptimal clustering results. Common methods for selecting K include the elbow method or silhouette analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04becf2b",
   "metadata": {},
   "source": [
    "## 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d690ed",
   "metadata": {},
   "source": [
    "The key difference between the k-means and k-medoids algorithms lies in how they choose the representative points for each cluster.\n",
    "\n",
    "**K-means Algorithm:**\n",
    "1. In the k-means algorithm, the representative point of each cluster is the mean (centroid) of the data points assigned to that cluster.\n",
    "2. It calculates the mean of the data points in each cluster and moves the cluster centroid to the new mean position.\n",
    "3. The algorithm minimizes the sum of squared distances between data points and their assigned cluster centroids.\n",
    "\n",
    "**Illustration of K-means:**\n",
    "Suppose we have a dataset with two-dimensional points and we want to cluster them into two clusters using k-means. Initially, the algorithm randomly places two cluster centroids (C1 and C2) in the data space.\n",
    "\n",
    "The algorithm then assigns each data point to the nearest centroid.\n",
    "\n",
    "Next, it calculates the mean of the data points in each cluster and moves the centroids to the new mean positions.\n",
    "\n",
    "The process repeats, with the centroids moving closer to the center of their respective clusters, until convergence is reached.\n",
    "\n",
    "**K-medoids Algorithm:**\n",
    "1. In the k-medoids algorithm, the representative point of each cluster is a data point (medoid) that minimizes the sum of distances to all other points in the cluster.\n",
    "2. It selects the medoid as the data point that minimizes the sum of distances to all other data points in the same cluster.\n",
    "3. The algorithm minimizes the sum of distances (not squared distances) between data points and their assigned medoids.\n",
    "\n",
    "**Illustration of K-medoids:**\n",
    "Using the same dataset, let's see how k-medoids works. Again, we want to cluster the points into two clusters.\n",
    "\n",
    "The algorithm starts with two random data points as the initial medoids.\n",
    "\n",
    "Then, it calculates the sum of distances between each data point and its assigned medoid. The data point with the minimum sum of distances becomes the new medoid for that cluster.\n",
    "\n",
    "The process continues until the medoids converge to stable positions.\n",
    "\n",
    "In summary, k-means uses the mean of the data points (centroid) as the representative point for each cluster, while k-medoids chooses an actual data point (medoid) that minimizes the sum of distances to other points in the same cluster. K-means minimizes squared distances, whereas k-medoids minimizes actual distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95056d",
   "metadata": {},
   "source": [
    "## 6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988a7bd",
   "metadata": {},
   "source": [
    "A dendrogram is a tree-like diagram used to visualize hierarchical clustering relationships among data points in a dataset. It is commonly used in data analysis and visualization to show how individual data points are grouped together based on their similarity or distance. Dendrograms are particularly useful when dealing with hierarchical clustering algorithms, which create a hierarchy of clusters where smaller clusters are nested within larger ones.\n",
    "\n",
    "Here's how to create a dendrogram:\n",
    "\n",
    "1. **Calculate Distance Matrix:** The first step is to compute the distance or similarity between each pair of data points in the dataset. This is typically done using a distance metric such as Euclidean distance or cosine similarity. The result is a distance matrix that shows the distance between all pairs of data points.\n",
    "\n",
    "2. **Linkage Method:** Next, you need to choose a linkage method, which determines how the distance between clusters is calculated as more data points are added to the cluster. Common linkage methods include single linkage, complete linkage, and average linkage.\n",
    "\n",
    "3. **Hierarchical Clustering:** Now, perform hierarchical clustering using the distance matrix and the chosen linkage method. Hierarchical clustering starts by considering each data point as its own cluster. Then, it iteratively merges the closest clusters based on the chosen linkage method, until all data points belong to a single cluster.\n",
    "\n",
    "4. **Dendrogram Visualization:** The final step is to visualize the hierarchical clustering results using a dendrogram. In the dendrogram, each data point is represented by a leaf node, and clusters are represented by branches in the tree. The height of each branch in the dendrogram represents the distance at which clusters are merged during the hierarchical clustering process. Longer branches indicate that the clusters were merged at a greater distance, indicating a lower similarity between the merged clusters.\n",
    "\n",
    "Here's a simple example:\n",
    "\n",
    "Suppose we have a dataset with five data points (A, B, C, D, E), and we want to create a dendrogram based on their pairwise Euclidean distances.\n",
    "\n",
    "1. Calculate Distance Matrix:\n",
    "   ```\n",
    "   | A | B | C | D | E |\n",
    "   ----------------------\n",
    "   A| 0 |   |   |   |   |\n",
    "   B| 3 | 0 |   |   |   |\n",
    "   C| 5 | 4 | 0 |   |   |\n",
    "   D| 7 | 6 | 8 | 0 |   |\n",
    "   E| 2 | 3 | 5 | 7 | 0 |\n",
    "   ```\n",
    "\n",
    "2. Linkage Method: Let's use complete linkage.\n",
    "\n",
    "3. Hierarchical Clustering:\n",
    "   ```\n",
    "   Step 1: Merge A and E -> (A, E)\n",
    "   Step 2: Merge B and E -> (B, E)\n",
    "   Step 3: Merge C and B -> (C, B, E)\n",
    "   Step 4: Merge D and C -> (D, C, B, E)\n",
    "   ```\n",
    "\n",
    "4. Dendrogram Visualization:\n",
    "   ```\n",
    "                 |------------|\n",
    "                 |      D     |\n",
    "           |-----|------------|\n",
    "           |     |            |\n",
    "           |     |       |----|\n",
    "           |     |       |  C |\n",
    "           |     |   |---|----|\n",
    "           |     |   |   |  B |\n",
    "           |     |   |   |----|\n",
    "           |-----|---|      E |\n",
    "                 |   |--------|\n",
    "                 |      A     |\n",
    "                 |------------|\n",
    "   ```\n",
    "\n",
    "In the dendrogram, the height of each branch indicates the distance at which clusters were merged. For example, the branch merging D and C has a height of 8, indicating that the distance between D and C was 8 when they were merged into a single cluster. Similarly, the branch merging A and E has a height of 2, indicating that the distance between A and E was 2 when they were merged.\n",
    "\n",
    "Dendrograms provide valuable insights into the hierarchical structure of data and can help identify natural clusters or groups within the dataset. They are widely used in various fields, including biology, data analysis, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b014b",
   "metadata": {},
   "source": [
    "## 7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445afbc",
   "metadata": {},
   "source": [
    "SSE stands for Sum of Squared Errors, also known as the within-cluster sum of squares. In the context of the k-means algorithm, SSE is a metric used to measure the quality of the clustering results.\n",
    "\n",
    "The k-means algorithm aims to partition a dataset into k clusters, where each data point belongs to the cluster whose center (centroid) is closest to it. The algorithm iteratively assigns data points to the nearest centroid and updates the centroids based on the mean of the data points in each cluster. The objective is to minimize the sum of squared distances between each data point and its assigned centroid, which is the SSE.\n",
    "\n",
    "Mathematically, the SSE is calculated as follows:\n",
    "\n",
    "SSE = Σᵢ Σⱼ (distance(dataᵢ, centroidⱼ))²\n",
    "\n",
    "where:\n",
    "- i is the index of data points,\n",
    "- j is the index of centroids (clusters),\n",
    "- distance(dataᵢ, centroidⱼ) is the distance between the data point dataᵢ and the centroid centroidⱼ.\n",
    "\n",
    "The k-means algorithm iteratively tries to find the centroids that minimize the SSE. It does so by performing the following steps:\n",
    "\n",
    "1. Initialize k centroids randomly.\n",
    "2. Assign each data point to the nearest centroid, forming k clusters.\n",
    "3. Calculate the new centroids as the mean of the data points in each cluster.\n",
    "4. Repeat steps 2 and 3 until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "The algorithm converges when the centroids stabilize, and the SSE reaches a local minimum. The final clustering is obtained when the SSE is minimized, and the data points are assigned to their respective clusters.\n",
    "\n",
    "The SSE is a measure of the compactness of the clusters; smaller SSE values indicate that the data points within each cluster are closer to their centroid, resulting in more cohesive and well-separated clusters. However, the k-means algorithm may converge to local minima, and the quality of the clustering results can be sensitive to the initial placement of centroids. Therefore, it is common practice to run the k-means algorithm multiple times with different initializations and choose the solution with the lowest SSE as the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205eb92",
   "metadata": {},
   "source": [
    "## 8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce84630",
   "metadata": {},
   "source": [
    "Here's a step-by-step explanation of the k-means algorithm:\n",
    "\n",
    "Step 1: Choose the number of clusters (k) that you want to create.\n",
    "\n",
    "Step 2: Initialize k centroids randomly. Each centroid is a point in the feature space.\n",
    "\n",
    "Step 3: Assign each data point to the nearest centroid. Each data point belongs to the cluster whose centroid is closest to it. This creates k clusters.\n",
    "\n",
    "Step 4: Calculate the new centroids as the mean of the data points in each cluster. The new centroid becomes the center of mass for all the data points in its cluster.\n",
    "\n",
    "Step 5: Repeat steps 3 and 4 until the centroids no longer change significantly or a maximum number of iterations is reached. Convergence is typically achieved when the centroids stabilize.\n",
    "\n",
    "Step 6: The final clustering is obtained when the centroids no longer change. Each data point is assigned to its nearest centroid, and you have k distinct clusters.\n",
    "\n",
    "Step 7: You can use the resulting clusters for various purposes, such as data analysis, pattern recognition, or making predictions.\n",
    "\n",
    "Note: The k-means algorithm is sensitive to the initial placement of centroids. To mitigate this issue, you can run the algorithm multiple times with different initializations and choose the solution with the lowest SSE (Sum of Squared Errors) as the final result. SSE is used as an evaluation metric, and it measures the quality of the clustering by summing the squared distances between each data point and its assigned centroid.\n",
    "\n",
    "The k-means algorithm is widely used for clustering data in various fields, such as image segmentation, customer segmentation, and pattern recognition. It is a simple yet effective unsupervised learning algorithm that can partition data into well-defined clusters based on their similarity to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09471a3e",
   "metadata": {},
   "source": [
    "## 9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53626bf7",
   "metadata": {},
   "source": [
    "In the context of hierarchical clustering, \"single link\" and \"complete link\" are two different linkage criteria used to measure the distance between clusters. These criteria determine how the distance between two clusters is computed based on the distances between their individual data points.\n",
    "\n",
    "1. Single Link (also known as Minimum Linkage):\n",
    "   In single link clustering, the distance between two clusters is defined as the shortest distance between any two data points in the two clusters. It considers the closest pair of data points, one from each cluster, and uses their distance as the distance between the clusters. This linkage tends to create long, elongated clusters and is sensitive to outliers or noise in the data.\n",
    "\n",
    "2. Complete Link (also known as Maximum Linkage):\n",
    "   In complete link clustering, the distance between two clusters is defined as the maximum distance between any two data points in the two clusters. It considers the farthest pair of data points, one from each cluster, and uses their distance as the distance between the clusters. This linkage tends to create compact, well-separated clusters and is less sensitive to outliers or noise in the data.\n",
    "\n",
    "Both single link and complete link are agglomerative hierarchical clustering methods, meaning that they start with each data point as its own cluster and then iteratively merge the closest or farthest clusters until all data points belong to a single cluster.\n",
    "\n",
    "The choice of linkage criteria can significantly impact the resulting clusters. Single link tends to form clusters with a \"chaining effect,\" where data points are connected in a chain-like manner. On the other hand, complete link tends to form clusters with a \"starburst effect,\" where each cluster is a well-separated, compact group of data points. The choice between single link and complete link (or other linkage criteria) depends on the nature of the data and the specific problem being addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e7e7e",
   "metadata": {},
   "source": [
    "## 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746320d3",
   "metadata": {},
   "source": [
    "The Apriori algorithm is a widely used technique in association rule mining, specifically for frequent itemset generation and mining frequent itemsets from transactional databases. It aids in the reduction of measurement overhead in business basket analysis by efficiently identifying frequent itemsets, which are sets of items that frequently occur together in transactions. By focusing only on the frequent itemsets, businesses can reduce the measurement overhead and avoid analyzing all possible item combinations, which can be computationally expensive.\n",
    "\n",
    "Let's consider an example to demonstrate how the Apriori algorithm helps in reducing measurement overhead:\n",
    "\n",
    "Suppose we have a retail store, and we want to analyze the purchasing patterns of customers to identify frequently co-occurring items in their baskets. We have transactional data where each transaction represents a customer's purchase history.\n",
    "\n",
    "Transaction 1: {Milk, Bread, Eggs}\n",
    "Transaction 2: {Milk, Bread, Butter}\n",
    "Transaction 3: {Milk, Eggs, Diapers}\n",
    "Transaction 4: {Bread, Butter, Diapers}\n",
    "Transaction 5: {Milk, Bread, Eggs, Butter}\n",
    "Transaction 6: {Milk, Bread, Butter, Diapers}\n",
    "\n",
    "The goal is to find frequent itemsets, such as sets of items that occur together frequently in transactions. For example, the frequent itemset {Milk, Bread, Butter} occurs in Transactions 2, 5, and 6. Using the Apriori algorithm, we can efficiently identify frequent itemsets.\n",
    "\n",
    "The Apriori algorithm works as follows:\n",
    "\n",
    "1. First, it scans the transactional data to identify the support of individual items. Support is the proportion of transactions that contain a particular item. Items with low support are pruned from further analysis.\n",
    "\n",
    "2. Next, it generates candidate itemsets of size 2 by combining items with high support from step 1. These candidate itemsets are then checked for support in the transactional data.\n",
    "\n",
    "3. The process continues iteratively to generate larger candidate itemsets until no more frequent itemsets can be found.\n",
    "\n",
    "The key advantage of the Apriori algorithm is that it avoids the need to generate all possible item combinations, significantly reducing the measurement overhead. Instead, it focuses on frequent itemsets, which are the most relevant for basket analysis and identifying associations between items.\n",
    "\n",
    "By using the Apriori algorithm, businesses can efficiently discover meaningful patterns in large transactional datasets, which can help them make data-driven decisions such as optimizing product placements, running targeted promotions, and improving customer recommendations. This reduction in measurement overhead allows businesses to perform basket analysis more effectively and with less computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
